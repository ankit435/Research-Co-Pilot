const jsondatadata=[
    {
        "id": "e99e94f3-5092-4480-a388-504d10543ae4",
        "title": "Evaluating computational models of explanation using human judgments",
        "abstract": "We evaluate four computational models of explanation in Bayesian networks by\ncomparing model predictions to human judgments. In two experiments, we present\nhuman participants with causal structures for which the models make divergent\npredictions and either solicit the best explanation for an observed event\n(Experiment 1) or have participants rate provided explanations for an observed\nevent (Experiment 2). Across two versions of two causal structures and across\nboth experiments, we find that the Causal Explanation Tree and Most Relevant\nExplanation models provide better fits to human data than either Most Probable\nExplanation or Explanation Tree models. We identify strengths and shortcomings\nof these models and what they can reveal about human explanation. We conclude\nby suggesting the value of pursuing computational and psychological\ninvestigations of explanation in parallel.",
        "authors": [
            "Michael Pacer",
            "Joseph Williams",
            "Xi Chen",
            "Tania Lombrozo",
            "Thomas Griffiths"
        ],
        "source": "Science direct",
        "url": "http://arxiv.org/abs/1309.6855v1",
        "pdf_url": "http://arxiv.org/pdf/1309.6855v1",
        "categories": [
            "Artificial Intelligence","BLOCK CHAIN"
        ],
        "publication_date": "2025-01-10",
        "created_at": "2025-01-16T09:33:47.637515Z",
        "is_bookmarked": false,
        "bookmark_id": null,
        "active_bookmarks_count": 0,
        "bookmarks": []
    },
    {
        "id": "f2ff2729-f974-4b09-a53d-0a7cefa983e2",
        "title": "Tighter Linear Program Relaxations for High Order Graphical Models",
        "abstract": "Graphical models with High Order Potentials (HOPs) have received considerable\ninterest in recent years. While there are a variety of approaches to inference\nin these models, nearly all of them amount to solving a linear program (LP)\nrelaxation with unary consistency constraints between the HOP and the\nindividual variables. In many cases, the resulting relaxations are loose, and\nin these cases the results of inference can be poor. It is thus desirable to\nlook for more accurate ways of performing inference in these models. In this\nwork, we study the LP relaxations that result from enforcing additional\nconsistency constraints between the HOP and the rest of the model. We address\ntheoretical questions about the strength of the resulting relaxations compared\nto the relaxations that arise in standard approaches, and we develop practical\nand efficient message passing algorithms for optimizing the LPs. Empirically,\nwe show that the LPs with additional consistency constraints lead to more\naccurate inference on some challenging problems that include a combination of\nlow order and high order terms.",
        "authors": [
            "Elad Mezuman",
            "Daniel Tarlow",
            "Amir Globerson",
            "Yair Weiss"
        ],
        "source": "Science direct",
        "url": "http://arxiv.org/abs/1309.6848v1",
        "pdf_url": "http://arxiv.org/pdf/1309.6848v1",
        "categories": [
            "Artificial Intelligence,ML"
        ],
        "publication_date": "2025-01-10",
        "created_at": "2025-01-16T09:33:47.567216Z",
        "is_bookmarked": false,
        "bookmark_id": null,
        "active_bookmarks_count": 0,
        "bookmarks": []
    },
    {
        "id": "aebb64e8-164d-45be-96aa-74841c4c390f",
        "title": "Learning Periodic Human Behaviour Models from Sparse Data for\n  Crowdsourcing Aid Delivery in Developing Countries",
        "abstract": "In many developing countries, half the population lives in rural locations,\nwhere access to essentials such as school materials, mosquito nets, and medical\nsupplies is restricted. We propose an alternative method of distribution (to\nstandard road delivery) in which the existing mobility habits of a local\npopulation are leveraged to deliver aid, which raises two technical challenges\nin the areas optimisation and learning. For optimisation, a standard Markov\ndecision process applied to this problem is intractable, so we provide an exact\nformulation that takes advantage of the periodicities in human location\nbehaviour. To learn such behaviour models from sparse data (i.e., cell tower\nobservations), we develop a Bayesian model of human mobility. Using real cell\ntower data of the mobility behaviour of 50,000 individuals in Ivory Coast, we\nfind that our model outperforms the state of the art approaches in mobility\nprediction by at least 25% (in held-out data likelihood). Furthermore, when\nincorporating mobility prediction with our MDP approach, we find a 81.3%\nreduction in total delivery time versus routine planning that minimises just\nthe number of participants in the solution path.",
        "authors": [
            "James McInerney",
            "Alex Rogers",
            "Nicholas R. Jennings"
        ],
        "source": "Science direct",
        "url": "http://arxiv.org/abs/1309.6846v1",
        "pdf_url": "http://arxiv.org/pdf/1309.6846v1",
        "categories": [
            "Artificial Intelligence"
        ],
        "publication_date": "2025-01-10",
        "created_at": "2025-01-16T09:33:47.490659Z",
        "is_bookmarked": false,
        "bookmark_id": null,
        "active_bookmarks_count": 0,
        "bookmarks": []
    },
    {
        "id": "382d6f7f-3d07-4b2c-a1a4-487b4f3a84be",
        "title": "On the Complexity of Strong and Epistemic Credal Networks",
        "abstract": "Credal networks are graph-based statistical models whose parameters take\nvalues in a set, instead of being sharply specified as in traditional\nstatistical models (e.g., Bayesian networks). The computational complexity of\ninferences on such models depends on the irrelevance/independence concept\nadopted. In this paper, we study inferential complexity under the concepts of\nepistemic irrelevance and strong independence. We show that inferences under\nstrong independence are NP-hard even in trees with ternary variables. We prove\nthat under epistemic irrelevance the polynomial time complexity of inferences\nin credal trees is not likely to extend to more general models (e.g. singly\nconnected networks). These results clearly distinguish networks that admit\nefficient inferences and those where inferences are most likely hard, and\nsettle several open questions regarding computational complexity.",
        "authors": [
            "Denis D. Maua",
            "Cassio Polpo de Campos",
            "Alessio Benavoli",
            "Alessandro Antonucci"
        ],
        "source": "Science direct",
        "url": "http://arxiv.org/abs/1309.6845v1",
        "pdf_url": "http://arxiv.org/pdf/1309.6845v1",
        "categories": [
            "Artificial Intelligence"
        ],
        "publication_date": "2025-01-10",
        "created_at": "2025-01-16T09:33:47.407409Z",
        "is_bookmarked": false,
        "bookmark_id": null,
        "active_bookmarks_count": 0,
        "bookmarks": []
    },
    {
        "id": "4f17b087-41c9-4bf8-86ac-e1dc01ea5aed",
        "title": "Evaluating Anytime Algorithms for Learning Optimal Bayesian Networks",
        "abstract": "Exact algorithms for learning Bayesian networks guarantee to find provably\noptimal networks. However, they may fail in difficult learning tasks due to\nlimited time or memory. In this research we adapt several anytime heuristic\nsearch-based algorithms to learn Bayesian networks. These algorithms find\nhigh-quality solutions quickly, and continually improve the incumbent solution\nor prove its optimality before resources are exhausted. Empirical results show\nthat the anytime window A* algorithm usually finds higher-quality, often\noptimal, networks more quickly than other approaches. The results also show\nthat, surprisingly, while generating networks with few parents per variable are\nstructurally simpler, they are harder to learn than complex generating networks\nwith more parents per variable.",
        "authors": [
            "Brandon Malone",
            "Changhe Yuan"
        ],
        "source": "Science direct",
        "url": "http://arxiv.org/abs/1309.6844v1",
        "pdf_url": "http://arxiv.org/pdf/1309.6844v1",
        "categories": [
            "Artificial Intelligence"
        ],
        "publication_date": "2025-01-10",
        "created_at": "2025-01-16T09:33:47.321098Z",
        "is_bookmarked": false,
        "bookmark_id": null,
        "active_bookmarks_count": 0,
        "bookmarks": []
    },
    {
        "id": "d31a8cbf-1c7e-432e-b7d7-aab028cd136f",
        "title": "A Sound and Complete Algorithm for Learning Causal Models from\n  Relational Data",
        "abstract": "The PC algorithm learns maximally oriented causal Bayesian networks. However,\nthere is no equivalent complete algorithm for learning the structure of\nrelational models, a more expressive generalization of Bayesian networks.\nRecent developments in the theory and representation of relational models\nsupport lifted reasoning about conditional independence. This enables a\npowerful constraint for orienting bivariate dependencies and forms the basis of\na new algorithm for learning structure. We present the relational causal\ndiscovery (RCD) algorithm that learns causal relational models. We prove that\nRCD is sound and complete, and we present empirical results that demonstrate\neffectiveness.",
        "authors": [
            "Marc Maier",
            "Katerina Marazopoulou",
            "David Arbour",
            "David Jensen"
        ],
        "source": "Science direct",
        "url": "http://arxiv.org/abs/1309.6843v1",
        "pdf_url": "http://arxiv.org/pdf/1309.6843v1",
        "categories": [
            "Artificial Intelligence"
        ],
        "publication_date": "2025-01-10",
        "created_at": "2025-01-16T09:33:47.240006Z",
        "is_bookmarked": false,
        "bookmark_id": null,
        "active_bookmarks_count": 0,
        "bookmarks": []
    },
    {
        "id": "c4fc7edd-b10e-4607-b5c6-bfc4c5135728",
        "title": "Causal Transportability of Experiments on Controllable Subsets of\n  Variables: z-Transportability",
        "abstract": "We introduce z-transportability, the problem of estimating the causal effect\nof a set of variables X on another set of variables Y in a target domain from\nexperiments on any subset of controllable variables Z where Z is an arbitrary\nsubset of observable variables V in a source domain. z-Transportability\ngeneralizes z-identifiability, the problem of estimating in a given domain the\ncausal effect of X on Y from surrogate experiments on a set of variables Z such\nthat Z is disjoint from X;. z-Transportability also generalizes\ntransportability which requires that the causal effect of X on Y in the target\ndomain be estimable from experiments on any subset of all observable variables\nin the source domain. We first generalize z-identifiability to allow cases\nwhere Z is not necessarily disjoint from X. Then, we establish a necessary and\nsufficient condition for z-transportability in terms of generalized\nz-identifiability and transportability. We provide a correct and complete\nalgorithm that determines whether a causal effect is z-transportable; and if it\nis, produces a transport formula, that is, a recipe for estimating the causal\neffect of X on Y in the target domain using information elicited from the\nresults of experimental manipulations of Z in the source domain and\nobservational data from the target domain. Our results also show that\ndo-calculus is complete for z-transportability.",
        "authors": [
            "Sanghack Lee",
            "Vasant Honavar"
        ],
        "source": "Science direct",
        "url": "http://arxiv.org/abs/1309.6842v1",
        "pdf_url": "http://arxiv.org/pdf/1309.6842v1",
        "categories": [
            "Artificial Intelligence"
        ],
        "publication_date": "2025-01-10",
        "created_at": "2025-01-16T09:33:47.154428Z",
        "is_bookmarked": false,
        "bookmark_id": null,
        "active_bookmarks_count": 0,
        "bookmarks": []
    },
    {
        "id": "d5364a50-27c4-4463-88d9-f7eeee0e5a56",
        "title": "Solving Limited-Memory Influence Diagrams Using Branch-and-Bound Search",
        "abstract": "A limited-memory influence diagram (LIMID) generalizes a traditional\ninfluence diagram by relaxing the assumptions of regularity and no-forgetting,\nallowing a wider range of decision problems to be modeled. Algorithms for\nsolving traditional influence diagrams are not easily generalized to solve\nLIMIDs, however, and only recently have exact algorithms for solving LIMIDs\nbeen developed. In this paper, we introduce an exact algorithm for solving\nLIMIDs that is based on branch-and-bound search. Our approach is related to the\napproach of solving an influence diagram by converting it to an equivalent\ndecision tree, with the difference that the LIMID is converted to a much\nsmaller decision graph that can be searched more efficiently.",
        "authors": [
            "Arindam Khaled",
            "Eric A. Hansen",
            "Changhe Yuan"
        ],
        "source": "Science direct",
        "url": "http://arxiv.org/abs/1309.6839v1",
        "pdf_url": "http://arxiv.org/pdf/1309.6839v1",
        "categories": [
            "Artificial Intelligence"
        ],
        "publication_date": "2025-01-10",
        "created_at": "2025-01-16T09:33:47.061215Z",
        "is_bookmarked": false,
        "bookmark_id": null,
        "active_bookmarks_count": 0,
        "bookmarks": []
    },
    {
        "id": "12dd7a52-27a0-42ea-86b4-ff0124b94e80",
        "title": "Discovering Cyclic Causal Models with Latent Variables: A General\n  SAT-Based Procedure",
        "abstract": "We present a very general approach to learning the structure of causal models\nbased on d-separation constraints, obtained from any given set of overlapping\npassive observational or experimental data sets. The procedure allows for both\ndirected cycles (feedback loops) and the presence of latent variables. Our\napproach is based on a logical representation of causal pathways, which permits\nthe integration of quite general background knowledge, and inference is\nperformed using a Boolean satisfiability (SAT) solver. The procedure is\ncomplete in that it exhausts the available information on whether any given\nedge can be determined to be present or absent, and returns \"unknown\"\notherwise. Many existing constraint-based causal discovery algorithms can be\nseen as special cases, tailored to circumstances in which one or more\nrestricting assumptions apply. Simulations illustrate the effect of these\nassumptions on discovery and how the present algorithm scales.",
        "authors": [
            "Antti Hyttinen",
            "Patrik O. Hoyer",
            "Frederick Eberhardt",
            "Matti Jarvisalo"
        ],
        "source": "Science direct",
        "url": "http://arxiv.org/abs/1309.6836v1",
        "pdf_url": "http://arxiv.org/pdf/1309.6836v1",
        "categories": [
            "Artificial Intelligence"
        ],
        "publication_date": "2025-01-10",
        "created_at": "2025-01-16T09:33:46.951927Z",
        "is_bookmarked": false,
        "bookmark_id": null,
        "active_bookmarks_count": 0,
        "bookmarks": []
    },
    {
        "id": "fe7921b9-f723-43cf-b1b4-e4e20967834d",
        "title": "Structured Message Passing",
        "abstract": "In this paper, we present structured message passing (SMP), a unifying\nframework for approximate inference algorithms that take advantage of\nstructured representations such as algebraic decision diagrams and sparse hash\ntables. These representations can yield significant time and space savings over\nthe conventional tabular representation when the message has several identical\nvalues (context-specific independence) or zeros (determinism) or both in its\nrange. Therefore, in order to fully exploit the power of structured\nrepresentations, we propose to artificially introduce context-specific\nindependence and determinism in the messages. This yields a new class of\npowerful approximate inference algorithms which includes popular algorithms\nsuch as cluster-graph Belief propagation (BP), expectation propagation and\nparticle BP as special cases. We show that our new algorithms introduce several\ninteresting bias-variance trade-offs. We evaluate these trade-offs empirically\nand demonstrate that our new algorithms are more accurate and scalable than\nstate-of-the-art techniques.",
        "authors": [
            "Vibhav Gogate",
            "Pedro Domingos"
        ],
        "source": "Science direct",
        "url": "http://arxiv.org/abs/1309.6832v1",
        "pdf_url": "http://arxiv.org/pdf/1309.6832v1",
        "categories": [
            "Artificial Intelligence"
        ],
        "publication_date": "2025-01-10",
        "created_at": "2025-01-16T09:33:46.870230Z",
        "is_bookmarked": false,
        "bookmark_id": null,
        "active_bookmarks_count": 0,
        "bookmarks": []
    }
]


export const data = jsondatadata;
export const researchData = jsondatadata;
